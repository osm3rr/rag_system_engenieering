# app.py

# IMPORTACIONES
import agent.config # Carga variables de entorno
import os
import streamlit as st
import uuid
from typing import Dict

# Importaciones de LangChain y LangGraph
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.memory import InMemorySaver
from langchain_core.messages import SystemMessage, AIMessage, HumanMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

# Importaciones de m贸dulos
from agent.state import GraphState
from agent.supervisor import DecisionSupervisor
from agent.specialists import agente_documento_01, agente_documento_02
from agent.config import embeddings_cohere, llm_gemini

# CONFIGURACIN Y LGICA DEL BACKEND
# L贸gica de construcci贸n del grafo.

# Directorios
VECTOR_STORE_BASE_PATH = "vector_stores"
PDF_TEMP_PATH = "temp_pdf"
os.makedirs(PDF_TEMP_PATH, exist_ok=True)
os.makedirs(VECTOR_STORE_BASE_PATH, exist_ok=True)

# Nodos del Grafo (supervisor_router, specialist_executor, supervisor_synthesizer)
def supervisor_router_node(state: GraphState) -> Dict:
    print(">> Supervisor (Router): Evaluando a qui茅n delegar...")
    llm_router = llm_gemini.with_structured_output(DecisionSupervisor)
    prompt = SystemMessage(content="""Eres un supervisor de un equipo de agentes. Analiza la consulta del usuario y enr煤tala al especialista correcto.

Reglas de Enrutamiento:
- 'agente_documento_01': Para preguntas sobre el directorio de proyectos de ingenier铆a.
- 'agente_documento_02': Para preguntas sobre el manual de procedimientos.
- Si el usuario te pide que verifiques o corrobores informaci贸n, DEBES enrutar la tarea al especialista correspondiente.
- 'finalizar': Si el usuario se despide.
        """)
    decision = llm_router.invoke([prompt] + state["messages"])
    print(f"   Decisi贸n: Delegar a '{decision.siguiente_agente}'")
    return {"siguiente_agente": decision.siguiente_agente}

def specialist_executor_node(state: GraphState) -> Dict:
    agent_name = state["siguiente_agente"]
    print(f">> Ejecutando Especialista: {agent_name}")
    specialist = {"agente_documento_01": agente_documento_01, "agente_documento_02": agente_documento_02}.get(agent_name)
    if not specialist: raise ValueError(f"Agente desconocido: {agent_name}")
    
    # Llamada s铆ncrona '.invoke()' con el historial completo
    result = specialist.invoke({"messages": state["messages"]})
    
    informe = result['messages'][-1].content
    print(f"   Informe del especialista: {informe[:300]}...")
    return {"informe_especialista": informe}

def supervisor_synthesizer_node(state: GraphState) -> Dict:
    print(">> Supervisor (Synthesizer): Formulando respuesta final...")
    informe = state["informe_especialista"]
    pregunta_usuario = state["messages"][-1].content
    print(f"   Pregunta del usuario: {pregunta_usuario}")
    template = ChatPromptTemplate.from_messages([
        ("system", """Eres un asistente de atenci贸n al cliente. Tu 煤nica funci贸n es tomar la informaci贸n t茅cnica recuperada por un especialista y presentarla de forma clara y amable al usuario.

Sigue estas reglas de forma OBLIGATORIA:
1.  Basa tu respuesta EXCLUSIVAMENTE en la "Informaci贸n recuperada por el especialista". NO uses conocimiento previo ni inventes datos.
2.  Si el informe del especialista indica que no se encontr贸 informaci贸n, tu 煤nica respuesta debe ser informar al usuario de que no se encontr贸 la informaci贸n en el documento.
3.  Si la pregunta del usuario es una continuaci贸n de la conversaci贸n (p. ej., "y sobre ella?", "dime m谩s"), usa el "Historial de la conversaci贸n" para entender el contexto, pero la informaci贸n para la respuesta DEBE provenir 煤nicamente del informe actual del especialista.
4.  Mant茅n siempre un tono servicial y dir铆gete al usuario por su nombre si lo conoces por el historial.
5.  NO justifiques ni inventes explicaciones si los datos del usuario contradicen los del especialista. Simplemente presenta la informaci贸n del especialista como la fuente de verdad. Por ejemplo, si el usuario dice "La puntuaci贸n es X" y el informe dice "La puntuaci贸n es Y", tu respuesta debe ser "Seg煤n mis datos, la puntuaci贸n es Y."."""),
        MessagesPlaceholder(variable_name="historial_chat"),
        ("user", f"Pregunta del usuario:\n{pregunta_usuario}\n\nInformaci贸n recuperada por el especialista:\n{informe}")
    ])
    synthesis_chain = template | llm_gemini
    response = synthesis_chain.invoke({
        "historial_chat": state["messages"],
        "informe_tecnico": informe
    })
    print(f"   Respuesta final generada: {response.content[:300]}...")
    return {"messages": [response]}


# Ensamblado del grafo
builder = StateGraph(GraphState)
builder.add_node("supervisor_router", supervisor_router_node)
builder.add_node("specialist_executor", specialist_executor_node)
builder.add_node("supervisor_synthesizer", supervisor_synthesizer_node)

builder.set_entry_point("supervisor_router")
builder.add_conditional_edges(
    "supervisor_router",
    lambda state: state["siguiente_agente"],
    {"agente_documento_01": "specialist_executor", "agente_documento_02": "specialist_executor", "finalizar": END}
)
builder.add_edge("specialist_executor", "supervisor_synthesizer")
builder.add_edge("supervisor_synthesizer", END)

checkpointer = InMemorySaver()
workflow = builder.compile(checkpointer=checkpointer)
print("Grafo Supervisor Multi-RAG (Delegaci贸n/S铆ntesis) compilado y listo para Streamlit.")

# LGICA DE LA INTERFAZ DE USUARIO CON STREAMLIT

# Configuraci贸n de la p谩gina
st.set_page_config(page_title="Ingelect", page_icon="", layout="wide")
st.title("Ingelect  - Engennier Assistant")

# Barra lateral para carga de documentos
with st.sidebar:
    st.header("Gesti贸n de Documentos")
    
    doc_type = st.selectbox("Elige el tipo de documento a cargar", ["documento_01", "documento_02"])
    
    uploaded_file = st.file_uploader("Sube un archivo PDF", type="pdf", key=f"uploader_{doc_type}")
    
    # Expander para agrupar la configuraci贸n avanzada
    with st.expander("Configuraci贸n de Procesamiento (Chunking)"):
        
        # st.number_input para el tama帽o del chunk
        chunk_size = st.number_input(
            "Tama帽o del Chunk", 
            min_value=100, 
            max_value=6000, 
            value=500, 
            step=50, # El valor que se incrementa/decrementa con los botones
            help="El n煤mero m谩ximo de caracteres en cada chunk. Afecta el detalle de la informaci贸n."
        )

        # st.number_input para el overlap del chunk
        chunk_overlap = st.number_input(
            "Solapamiento de Chunks", 
            min_value=0, 
            max_value=2000, 
            value=125, 
            step=25, # El salto m谩s peque帽o aqu铆
            help="Cu谩ntos caracteres se superponen entre chunks para mantener el contexto."
        )
        # ---------------------

    if st.button(f"Procesar Documento: {doc_type}"):
        if uploaded_file is not None:
            with st.spinner(f"Procesando '{uploaded_file.name}'..."):
                
                # Guardado temporal del archivo
                temp_file_path = os.path.join(PDF_TEMP_PATH, uploaded_file.name)
                with open(temp_file_path, "wb") as f:
                    f.write(uploaded_file.getbuffer())
                
                # L贸gica de procesamiento
                loader = PyPDFLoader(temp_file_path)
                text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
                chunks = text_splitter.split_documents(loader.load_and_split())
                
                if chunks:
                    ruta_vector_store = os.path.join(VECTOR_STORE_BASE_PATH, doc_type)
                    vector_store = FAISS.from_documents(chunks, embeddings_cohere)
                    vector_store.save_local(ruta_vector_store)
                    st.success(f"隆'{doc_type}' procesado! ({len(chunks)} chunks)")
                else:
                    st.error("No se pudo extraer texto del PDF.")

                os.remove(temp_file_path)
        else:
            st.warning("Por favor, sube un archivo PDF.")

# Interfaz de Chat
st.divider()

# Inicializaci贸n del thread_id y el historial de mensajes en el estado de la sesi贸n de Streamlit
if "thread_id" not in st.session_state:
    st.session_state.thread_id = str(uuid.uuid4())
if "messages" not in st.session_state:
    st.session_state.messages = []

# Mostrar mensajes del historial guardado en la sesi贸n de Streamlit
for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

# Entrada de usuario
if prompt := st.chat_input("Pregunta sobre proyectos (doc_01) o procedimientos (doc_02)..."):
    # A帽adir el mensaje del usuario a la lista de la UI
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)

    with st.chat_message("assistant"):
        with st.spinner("Pensando..."):
            
            #    Se construye el historial de entrada para el grafo.
            #    Se convierte el historial de diccionarios de la UI al formato que LangGraph espera (HumanMessage, AIMessage).
            #    Esto asegura que el grafo reciba TODA la conversaci贸n anterior.
            input_messages = []
            for msg in st.session_state.messages:
                if msg["role"] == "user":
                    input_messages.append(HumanMessage(content=msg["content"]))
                else:
                    input_messages.append(AIMessage(content=msg["content"]))

            #    Se prepara el input y la configuraci贸n para la llamada.
            #    El input contiene el historial completo.
            config = {"configurable": {"thread_id": st.session_state.thread_id}}
            input_data = {"messages": input_messages}
            
            #    Se invoca el workflow con el historial completo.
            final_state = workflow.invoke(input_data, config)
            
            #    Se obtiene solo la 煤ltima respuesta (la que acaba de generar el agente).
            response_content = final_state["messages"][-1].content
            
            #    Se muestra la respuesta y se a帽ade al historial de la UI.
            st.write(response_content)
            st.session_state.messages.append({"role": "assistant", "content": response_content})